---
name: OpenAI GPT-5.2 & Embeddings Integration
status: closed
created: 2025-12-25T18:50:10Z
updated: 2025-12-25T18:58:45Z
github:
depends_on: [13, 22]
parallel: false
conflicts_with: []
---

# Task: OpenAI GPT-5.2 & Embeddings Integration

## Description

Integrate OpenAI GPT-5.2 for all chat interactions and text-embedding-3-small for RAG embeddings. This replaces the originally planned Anthropic Claude integration. The task includes setting up the OpenAI client, implementing streaming chat completions, and building the embedding pipeline for intake data and conversation summaries.

## Acceptance Criteria

- [ ] OpenAI SDK configured with API key and GPT-5.2 model
- [ ] Chat AI Service uses GPT-5.2 for all conversation phases (exploration, guidance, shared chat)
- [ ] Streaming responses work via Server-Sent Events (SSE)
- [ ] Embedding Service generates vectors using text-embedding-3-small
- [ ] Intake survey responses embedded and stored in SurrealDB
- [ ] Conversation summaries embedded for pattern matching
- [ ] Vector similarity search working for RAG context retrieval
- [ ] Error handling for rate limits and API failures
- [ ] Cost monitoring/logging for token usage

## Technical Details

**OpenAI Configuration**:
```typescript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Chat completion with GPT-5.2
const response = await openai.chat.completions.create({
  model: 'gpt-5.2',
  messages: conversationMessages,
  stream: true,
  max_tokens: 4096,
});

// Embeddings with text-embedding-3-small
const embedding = await openai.embeddings.create({
  model: 'text-embedding-3-small',
  input: textToEmbed,
});
```

**GPT-5.2 Model Details**:
- Pricing: $1.75/M input tokens, $14/M output tokens
- Context window: 400,000 tokens
- Max output: 128,000 tokens
- Knowledge cutoff: August 31, 2025
- Modes: "instant" and "thinking"

**Embedding Model Details**:
- Model: text-embedding-3-small
- Dimensions: 1536
- Pricing: ~$0.02/M tokens (negligible cost)

**Backend Structure**:
```
src/services/
├── openai-client.ts      # OpenAI SDK singleton
├── chat-ai.ts            # Chat completions service (GPT-5.2)
├── embedding.ts          # Embedding generation service
└── rag-context.ts        # Vector search & context retrieval
```

**Chat AI Service** (`src/services/chat-ai.ts`):
```typescript
interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export async function respondToMessage(
  sessionId: string,
  userMessage: string,
  systemPrompt: string,
  conversationHistory: ChatMessage[]
): AsyncGenerator<string> {
  const messages: ChatMessage[] = [
    { role: 'system', content: systemPrompt },
    ...conversationHistory,
    { role: 'user', content: userMessage }
  ];

  const stream = await openai.chat.completions.create({
    model: 'gpt-5.2',
    messages,
    stream: true,
  });

  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content;
    if (content) yield content;
  }
}
```

**Embedding Service** (`src/services/embedding.ts`):
```typescript
export async function generateEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  return response.data[0].embedding;
}

export async function embedAndStore(
  text: string,
  metadata: { type: string; referenceId: string }
): Promise<void> {
  const embedding = await generateEmbedding(text);
  await db.query(`
    CREATE embeddings CONTENT {
      vector: $embedding,
      text: $text,
      type: $type,
      reference_id: $referenceId,
      created_at: time::now()
    }
  `, { embedding, text, type: metadata.type, referenceId: metadata.referenceId });
}
```

**SurrealDB Embeddings Schema**:
```surrealql
-- Embeddings table for RAG
DEFINE TABLE embeddings SCHEMAFULL;
DEFINE FIELD vector ON embeddings TYPE array<float>;
DEFINE FIELD text ON embeddings TYPE string;
DEFINE FIELD type ON embeddings TYPE string ASSERT $value IN ['intake', 'conversation_summary', 'conflict'];
DEFINE FIELD reference_id ON embeddings TYPE string;
DEFINE FIELD created_at ON embeddings TYPE datetime DEFAULT time::now();

-- Vector index for similarity search
DEFINE INDEX embedding_vector_idx ON embeddings FIELDS vector MTREE DIMENSION 1536;
```

**Vector Similarity Search**:
```typescript
export async function findSimilarContext(
  queryText: string,
  type: string,
  limit: number = 5
): Promise<RAGContext[]> {
  const queryEmbedding = await generateEmbedding(queryText);

  const results = await db.query(`
    SELECT *, vector::similarity::cosine(vector, $query) AS score
    FROM embeddings
    WHERE type = $type
    ORDER BY score DESC
    LIMIT $limit
  `, { query: queryEmbedding, type, limit });

  return results;
}
```

**Environment Variables**:
```
# OpenAI
OPENAI_API_KEY=sk-...
OPENAI_CHAT_MODEL=gpt-5.2
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

**SSE Streaming Endpoint** (`src/routes/chat.ts`):
```typescript
router.post('/conversations/:id/messages', async (req, res) => {
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');

  const { message } = req.body;
  const sessionId = req.params.id;

  try {
    const stream = respondToMessage(sessionId, message, systemPrompt, history);

    for await (const chunk of stream) {
      res.write(`data: ${JSON.stringify({ content: chunk })}\n\n`);
    }

    res.write(`data: ${JSON.stringify({ done: true })}\n\n`);
    res.end();
  } catch (error) {
    res.write(`data: ${JSON.stringify({ error: error.message })}\n\n`);
    res.end();
  }
});
```

**Token Usage Logging**:
```typescript
// Log token usage for cost monitoring
export async function logTokenUsage(
  sessionId: string,
  promptTokens: number,
  completionTokens: number,
  model: string
): Promise<void> {
  const inputCost = (promptTokens / 1_000_000) * 1.75;  // GPT-5.2 input rate
  const outputCost = (completionTokens / 1_000_000) * 14; // GPT-5.2 output rate

  await db.query(`
    CREATE token_usage CONTENT {
      session_id: $sessionId,
      prompt_tokens: $promptTokens,
      completion_tokens: $completionTokens,
      model: $model,
      cost_usd: $cost,
      created_at: time::now()
    }
  `, { sessionId, promptTokens, completionTokens, model, cost: inputCost + outputCost });
}
```

## Dependencies

- [ ] Task #13 (Database & Auth Foundation) - SurrealDB schema
- [ ] Task #22 (Chat Infrastructure) - WebSocket/SSE setup
- [ ] OpenAI API key with GPT-5.2 access
- [ ] openai npm package (^4.x)

## Effort Estimate

- **Size**: M
- **Hours**: 24-32 hours (3-4 days)
- **Parallel**: false (blocks guidance synthesis and pattern recognition)

## Definition of Done

- [ ] OpenAI SDK initialized and connected
- [ ] GPT-5.2 chat completions working with streaming
- [ ] System prompts loading correctly for each conversation phase
- [ ] text-embedding-3-small generating 1536-dimension vectors
- [ ] Embeddings stored in SurrealDB with vector index
- [ ] Vector similarity search returning relevant context
- [ ] SSE streaming endpoint tested with frontend
- [ ] Token usage logged for cost monitoring
- [ ] Error handling for rate limits (429) and API errors
- [ ] Unit tests for embedding service
- [ ] Integration tests for chat flow with mocked OpenAI responses
